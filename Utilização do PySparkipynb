{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONchJ2pcOf49uRHd054XRA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uH7dAQsVIBXo","executionInfo":{"status":"ok","timestamp":1741963836896,"user_tz":180,"elapsed":19143,"user":{"displayName":"Matheus Ferreira","userId":"00864485544663136892"}},"outputId":"07d51cc5-d90d-4a78-ed5b-1d584ded3821"},"outputs":[{"output_type":"stream","name":"stdout","text":["<SparkContext master=local[*] appName=pyspark-shell>\n","3.5.5\n"]}],"source":["from pyspark import SparkContext\n","spark_contexto = SparkContext()\n","print(spark_contexto)\n","print(spark_contexto.version)"]},{"cell_type":"markdown","source":["realizamos o primeiro passo para trabalhar com o Spark DataFrames, que foi a criação do SparkContext para estabelecermos uma conexão com o cluster. Agora, precisamos criar um SparkSession, que é a interface com essa conexão. Para isso, vamos executar o código abaixo para criar uma seção no Spark (SparkSession):"],"metadata":{"id":"OiNEMMGwIJki"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate() # Create my_spark\n","print(spark) # Print my_spark\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHObZ1FcIKHH","executionInfo":{"status":"ok","timestamp":1741963841242,"user_tz":180,"elapsed":437,"user":{"displayName":"Matheus Ferreira","userId":"00864485544663136892"}},"outputId":"257528ff-e315-49f3-b659-3856f3d46dc3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["<pyspark.sql.session.SparkSession object at 0x7a0c712ad850>\n"]}]},{"cell_type":"markdown","source":["Na primeira linha, importamos o SparkSession a partir do pyspark.sql. Na segunda linha, criamos uma seção do Spark. Na terceira linha, imprimimos o objeto Spark"],"metadata":{"id":"EC1qZAe5IjyB"}},{"cell_type":"code","source":["dataset = spark.read.csv('/content/sample_data/california_housing_test.csv',inferSchema=True, header =True)"],"metadata":{"id":"GWYmIecWIxok","executionInfo":{"status":"ok","timestamp":1741963901625,"user_tz":180,"elapsed":11893,"user":{"displayName":"Matheus Ferreira","userId":"00864485544663136892"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Agora, vamos ler os dados de um arquivo disponível no diretório “sample_data” do Google Colab. No nosso caso, utilizamos o arquivo “california_housing_test.csv”. A seguir, podemos analisar o código de leitura dos dados do arquivo:"],"metadata":{"id":"3VldDvv-Iy7B"}},{"cell_type":"code","source":["dataset.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OU_WgP1VI-CB","executionInfo":{"status":"ok","timestamp":1741963938902,"user_tz":180,"elapsed":449,"user":{"displayName":"Matheus Ferreira","userId":"00864485544663136892"}},"outputId":"7115f179-22a7-4e32-fae6-5af8e2013d66"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0)"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["executamos o código a seguir, que vai exibir o cabeçalho das colunas e o conteúdo da primeira linha"],"metadata":{"id":"wp8fAgfKI_RB"}},{"cell_type":"code","source":["dataset.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v233sWadJJHT","executionInfo":{"status":"ok","timestamp":1741963974202,"user_tz":180,"elapsed":1085,"user":{"displayName":"Matheus Ferreira","userId":"00864485544663136892"}},"outputId":"af402912-f33a-4f81-edf7-890c9e0d0e8b"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3000"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Para obtermos a quantidade de linhas no dataset"],"metadata":{"id":"1v2C_STWJKCn"}},{"cell_type":"code","source":["dataset.createOrReplaceTempView('tabela_temporaria')\n","print(spark.catalog.listTables())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0IT2g06JOLe","executionInfo":{"status":"ok","timestamp":1741964025000,"user_tz":180,"elapsed":1099,"user":{"displayName":"Matheus Ferreira","userId":"00864485544663136892"}},"outputId":"0e20c167-f67a-4ad4-fd65-ee77e2a562c7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[Table(name='tabela_temporaria', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"]}]},{"cell_type":"markdown","source":["Agora vamos criar uma tabela SQL temporária com os dados do “dataset”"],"metadata":{"id":"KSBqA_HBJSMh"}},{"cell_type":"code","source":["query = 'FROM tabela_temporaria SELECT longitude, latitude LIMIT 3'\n","saida = spark.sql(query)\n","saida.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqFcaEBMJcvz","executionInfo":{"status":"ok","timestamp":1741964062608,"user_tz":180,"elapsed":665,"user":{"displayName":"Matheus Ferreira","userId":"00864485544663136892"}},"outputId":"26094523-683b-48f2-a5ea-107c55192847"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+--------+\n","|longitude|latitude|\n","+---------+--------+\n","|  -122.05|   37.37|\n","|   -118.3|   34.26|\n","|  -117.81|   33.78|\n","+---------+--------+\n","\n"]}]},{"cell_type":"markdown","source":["O próximo passo consiste em fazer uma consulta SQL. Para isso, vamos selecionar apenas três registros com os dados referentes às colunas “longitude” e “latitude”"],"metadata":{"id":"LBhEsL8dJein"}}]}